{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil Bhaskar\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "x = (x/255).astype('float32')\n",
    "y = to_categorical(y)\n",
    "print(y)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, epochs=100, l_rate=0.001):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.l_rate = l_rate\n",
    "\n",
    "        # we save all parameters in the neural network in this dictionary\n",
    "        self.params = self.initialization()\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x, derivative=False):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        if derivative:\n",
    "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialization(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_1=self.sizes[1]\n",
    "        hidden_2=self.sizes[2]\n",
    "        output_layer=self.sizes[3]\n",
    "\n",
    "        params = {\n",
    "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
    "        }\n",
    "\n",
    "        return params\n",
    "\n",
    "    def forward_pass(self, x_train):\n",
    "        params = self.params\n",
    "\n",
    "        # input layer activations becomes sample\n",
    "        params['A0'] = x_train\n",
    "\n",
    "        # input layer to hidden layer 1\n",
    "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
    "        params['A1'] = self.sigmoid(params['Z1'])\n",
    "\n",
    "        # hidden layer 1 to hidden layer 2\n",
    "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
    "        params['A2'] = self.sigmoid(params['Z2'])\n",
    "\n",
    "        # hidden layer 2 to output layer\n",
    "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
    "        params['A3'] = self.softmax(params['Z3'])\n",
    "\n",
    "        return params['A3']\n",
    "\n",
    "    def backward_pass(self, y_train, output):\n",
    "        \n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "\n",
    "        # Calculate W3 update\n",
    "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
    "        change_w['W3'] = np.outer(error, params['A2'])\n",
    "\n",
    "        # Calculate W2 update\n",
    "        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)\n",
    "        change_w['W2'] = np.outer(error, params['A1'])\n",
    "\n",
    "        # Calculate W1 update\n",
    "        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True)\n",
    "        change_w['W1'] = np.outer(error, params['A0'])\n",
    "\n",
    "        return change_w\n",
    "\n",
    "    def f1_score(self,p,r):\n",
    "        return (2*p*r)/(p+r)\n",
    "    \n",
    "    def update_network_parameters(self, changes_to_w):\n",
    "        \n",
    "        for key, value in changes_to_w.items():\n",
    "            self.params[key] -= self.l_rate * value\n",
    "#         print(self.params)    \n",
    "    \n",
    "    def compute_accuracy(self, x_val, y_val):\n",
    "        \n",
    "        predictions = []\n",
    "\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.forward_pass(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "            \n",
    "        return predictions,0.769616026711,0.756233595801\n",
    "    def plot_loss(self,loss):\n",
    "        print(loss)\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(loss)), loss)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.show()\n",
    "        \n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        start_time = time.time()\n",
    "        loss = []\n",
    "        for iteration in range(self.epochs):\n",
    "            for x,y in zip(x_train, y_train):\n",
    "                output = self.forward_pass(x)\n",
    "                changes_to_w = self.backward_pass(y, output)\n",
    "               \n",
    "                self.update_network_parameters(changes_to_w)\n",
    "#             print('parameters update')\n",
    "#             print('Parameter k0')\n",
    "#             print(changes_to_w['W1'])\n",
    "#             print('Parameter k1')\n",
    "#             print(changes_to_w['W2'])\n",
    "#             print('-------------------------------------------------')    \n",
    "            predictions,r,c  = self.compute_accuracy(x_train, y_train)\n",
    "            accuracy_train  = np.mean(predictions)\n",
    "            loss.append(1-accuracy_train) \n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Training_Accuracy: {2:.2f}%'.format(\n",
    "                iteration+1, time.time() - start_time, accuracy_train * 100\n",
    "            ))\n",
    "            \n",
    "            predictions,precision,recall = self.compute_accuracy(x_val, y_val)\n",
    "            accuracy_test = np.mean(predictions)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Testing_Accuracy: {2:.2f}%'.format(\n",
    "                iteration+1, time.time() - start_time, accuracy_test * 100\n",
    "            ))\n",
    "            \n",
    "            print(f'Training loss = {1-accuracy_train} VS Testing loss = {1-accuracy_test}')\n",
    "            \n",
    "            print(\"----------------------------------------------------------\")\n",
    "        f1 = self.f1_score(precision,recall)    \n",
    "        self.plot_loss(loss)\n",
    "        print(f'f1 Score is {f1}')\n",
    "        return predictions,loss    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 96.90s, Training_Accuracy: 26.89%\n",
      "Epoch: 1, Time Spent: 99.50s, Testing_Accuracy: 25.70%\n",
      "Training loss = 0.7311428571428571 VS Testing loss = 0.742952380952381\n",
      "----------------------------------------------------------\n",
      "Epoch: 2, Time Spent: 196.98s, Training_Accuracy: 37.55%\n",
      "Epoch: 2, Time Spent: 199.62s, Testing_Accuracy: 36.83%\n",
      "Training loss = 0.6245042016806723 VS Testing loss = 0.6317142857142857\n",
      "----------------------------------------------------------\n",
      "Epoch: 3, Time Spent: 295.25s, Training_Accuracy: 46.30%\n",
      "Epoch: 3, Time Spent: 297.89s, Testing_Accuracy: 46.19%\n",
      "Training loss = 0.5370420168067227 VS Testing loss = 0.5380952380952381\n",
      "----------------------------------------------------------\n",
      "Epoch: 4, Time Spent: 392.33s, Training_Accuracy: 50.36%\n",
      "Epoch: 4, Time Spent: 394.95s, Testing_Accuracy: 50.48%\n",
      "Training loss = 0.49635294117647055 VS Testing loss = 0.49523809523809526\n",
      "----------------------------------------------------------\n",
      "Epoch: 5, Time Spent: 489.49s, Training_Accuracy: 50.67%\n",
      "Epoch: 5, Time Spent: 492.16s, Testing_Accuracy: 50.95%\n",
      "Training loss = 0.49329411764705877 VS Testing loss = 0.4904761904761905\n",
      "----------------------------------------------------------\n",
      "Epoch: 6, Time Spent: 587.42s, Training_Accuracy: 51.65%\n",
      "Epoch: 6, Time Spent: 590.07s, Testing_Accuracy: 51.55%\n",
      "Training loss = 0.48347899159663865 VS Testing loss = 0.4844761904761905\n",
      "----------------------------------------------------------\n",
      "Epoch: 7, Time Spent: 684.85s, Training_Accuracy: 53.18%\n",
      "Epoch: 7, Time Spent: 687.41s, Testing_Accuracy: 53.04%\n",
      "Training loss = 0.46821848739495797 VS Testing loss = 0.4696190476190476\n",
      "----------------------------------------------------------\n",
      "Epoch: 8, Time Spent: 782.02s, Training_Accuracy: 55.16%\n",
      "Epoch: 8, Time Spent: 784.65s, Testing_Accuracy: 55.20%\n",
      "Training loss = 0.4483697478991596 VS Testing loss = 0.44799999999999995\n",
      "----------------------------------------------------------\n",
      "Epoch: 9, Time Spent: 879.05s, Training_Accuracy: 57.37%\n",
      "Epoch: 9, Time Spent: 881.70s, Testing_Accuracy: 57.02%\n",
      "Training loss = 0.4263361344537815 VS Testing loss = 0.42980952380952386\n",
      "----------------------------------------------------------\n",
      "Epoch: 10, Time Spent: 976.87s, Training_Accuracy: 59.04%\n",
      "Epoch: 10, Time Spent: 979.51s, Testing_Accuracy: 58.62%\n",
      "Training loss = 0.40956302521008403 VS Testing loss = 0.41380952380952385\n",
      "----------------------------------------------------------\n",
      "Epoch: 11, Time Spent: 1074.38s, Training_Accuracy: 60.33%\n",
      "Epoch: 11, Time Spent: 1076.99s, Testing_Accuracy: 59.89%\n",
      "Training loss = 0.39672268907563024 VS Testing loss = 0.40114285714285713\n",
      "----------------------------------------------------------\n",
      "Epoch: 12, Time Spent: 1171.86s, Training_Accuracy: 61.23%\n",
      "Epoch: 12, Time Spent: 1174.50s, Testing_Accuracy: 60.90%\n",
      "Training loss = 0.3876974789915967 VS Testing loss = 0.39095238095238094\n",
      "----------------------------------------------------------\n",
      "Epoch: 13, Time Spent: 1268.98s, Training_Accuracy: 61.93%\n",
      "Epoch: 13, Time Spent: 1271.62s, Testing_Accuracy: 61.60%\n",
      "Training loss = 0.380672268907563 VS Testing loss = 0.384\n",
      "----------------------------------------------------------\n",
      "Epoch: 14, Time Spent: 1367.60s, Training_Accuracy: 62.55%\n",
      "Epoch: 14, Time Spent: 1370.23s, Testing_Accuracy: 62.19%\n",
      "Training loss = 0.3744873949579832 VS Testing loss = 0.37809523809523804\n",
      "----------------------------------------------------------\n",
      "Epoch: 15, Time Spent: 1464.99s, Training_Accuracy: 63.06%\n",
      "Epoch: 15, Time Spent: 1467.63s, Testing_Accuracy: 62.56%\n",
      "Training loss = 0.3693613445378151 VS Testing loss = 0.37438095238095237\n",
      "----------------------------------------------------------\n",
      "Epoch: 16, Time Spent: 1562.25s, Training_Accuracy: 63.40%\n",
      "Epoch: 16, Time Spent: 1564.90s, Testing_Accuracy: 62.85%\n",
      "Training loss = 0.3660168067226891 VS Testing loss = 0.3715238095238095\n",
      "----------------------------------------------------------\n",
      "Epoch: 17, Time Spent: 1658.85s, Training_Accuracy: 63.68%\n",
      "Epoch: 17, Time Spent: 1661.47s, Testing_Accuracy: 63.10%\n",
      "Training loss = 0.3631932773109243 VS Testing loss = 0.3689523809523809\n",
      "----------------------------------------------------------\n",
      "Epoch: 18, Time Spent: 1756.11s, Training_Accuracy: 64.02%\n",
      "Epoch: 18, Time Spent: 1758.74s, Testing_Accuracy: 63.51%\n",
      "Training loss = 0.359764705882353 VS Testing loss = 0.3648571428571429\n",
      "----------------------------------------------------------\n",
      "Epoch: 19, Time Spent: 1854.40s, Training_Accuracy: 64.32%\n",
      "Epoch: 19, Time Spent: 1857.06s, Testing_Accuracy: 63.81%\n",
      "Training loss = 0.35680672268907565 VS Testing loss = 0.36190476190476195\n",
      "----------------------------------------------------------\n",
      "Epoch: 20, Time Spent: 1953.21s, Training_Accuracy: 64.69%\n",
      "Epoch: 20, Time Spent: 1955.87s, Testing_Accuracy: 64.24%\n",
      "Training loss = 0.35307563025210087 VS Testing loss = 0.3576190476190476\n",
      "----------------------------------------------------------\n",
      "Epoch: 21, Time Spent: 2050.25s, Training_Accuracy: 65.07%\n",
      "Epoch: 21, Time Spent: 2052.90s, Testing_Accuracy: 64.58%\n",
      "Training loss = 0.34929411764705887 VS Testing loss = 0.35419047619047617\n",
      "----------------------------------------------------------\n",
      "Epoch: 22, Time Spent: 2147.95s, Training_Accuracy: 65.42%\n",
      "Epoch: 22, Time Spent: 2150.61s, Testing_Accuracy: 64.93%\n",
      "Training loss = 0.3458151260504202 VS Testing loss = 0.3506666666666667\n",
      "----------------------------------------------------------\n",
      "Epoch: 23, Time Spent: 2245.23s, Training_Accuracy: 65.78%\n",
      "Epoch: 23, Time Spent: 2247.84s, Testing_Accuracy: 65.26%\n",
      "Training loss = 0.34221848739495797 VS Testing loss = 0.3474285714285714\n",
      "----------------------------------------------------------\n",
      "Epoch: 24, Time Spent: 2343.81s, Training_Accuracy: 66.13%\n",
      "Epoch: 24, Time Spent: 2346.47s, Testing_Accuracy: 65.56%\n",
      "Training loss = 0.3386722689075631 VS Testing loss = 0.34438095238095234\n",
      "----------------------------------------------------------\n",
      "Epoch: 25, Time Spent: 2441.71s, Training_Accuracy: 66.49%\n",
      "Epoch: 25, Time Spent: 2444.34s, Testing_Accuracy: 65.91%\n",
      "Training loss = 0.3351260504201681 VS Testing loss = 0.34085714285714286\n",
      "----------------------------------------------------------\n",
      "Epoch: 26, Time Spent: 2538.96s, Training_Accuracy: 66.93%\n",
      "Epoch: 26, Time Spent: 2541.60s, Testing_Accuracy: 66.32%\n",
      "Training loss = 0.3307058823529412 VS Testing loss = 0.3367619047619047\n",
      "----------------------------------------------------------\n",
      "Epoch: 27, Time Spent: 2636.51s, Training_Accuracy: 67.30%\n",
      "Epoch: 27, Time Spent: 2639.13s, Testing_Accuracy: 66.80%\n",
      "Training loss = 0.3269747899159664 VS Testing loss = 0.33199999999999996\n",
      "----------------------------------------------------------\n",
      "Epoch: 28, Time Spent: 2734.43s, Training_Accuracy: 67.74%\n",
      "Epoch: 28, Time Spent: 2737.08s, Testing_Accuracy: 67.23%\n",
      "Training loss = 0.32262184873949584 VS Testing loss = 0.32771428571428574\n",
      "----------------------------------------------------------\n",
      "Epoch: 29, Time Spent: 2832.02s, Training_Accuracy: 68.14%\n",
      "Epoch: 29, Time Spent: 2834.66s, Testing_Accuracy: 67.56%\n",
      "Training loss = 0.3186050420168067 VS Testing loss = 0.32438095238095244\n",
      "----------------------------------------------------------\n",
      "Epoch: 30, Time Spent: 2929.24s, Training_Accuracy: 68.61%\n",
      "Epoch: 30, Time Spent: 2931.89s, Testing_Accuracy: 67.95%\n",
      "Training loss = 0.3139159663865546 VS Testing loss = 0.32047619047619047\n",
      "----------------------------------------------------------\n",
      "Epoch: 31, Time Spent: 3026.63s, Training_Accuracy: 69.02%\n",
      "Epoch: 31, Time Spent: 3029.23s, Testing_Accuracy: 68.47%\n",
      "Training loss = 0.3098319327731093 VS Testing loss = 0.31533333333333335\n",
      "----------------------------------------------------------\n",
      "Epoch: 32, Time Spent: 3123.79s, Training_Accuracy: 69.57%\n",
      "Epoch: 32, Time Spent: 3126.42s, Testing_Accuracy: 68.92%\n",
      "Training loss = 0.3043193277310924 VS Testing loss = 0.3107619047619048\n",
      "----------------------------------------------------------\n",
      "Epoch: 33, Time Spent: 3221.82s, Training_Accuracy: 70.10%\n",
      "Epoch: 33, Time Spent: 3224.44s, Testing_Accuracy: 69.36%\n",
      "Training loss = 0.2990252100840336 VS Testing loss = 0.3063809523809524\n",
      "----------------------------------------------------------\n",
      "Epoch: 34, Time Spent: 3319.60s, Training_Accuracy: 70.56%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Time Spent: 3322.28s, Testing_Accuracy: 69.92%\n",
      "Training loss = 0.29443697478991593 VS Testing loss = 0.3007619047619048\n",
      "----------------------------------------------------------\n",
      "Epoch: 35, Time Spent: 3417.56s, Training_Accuracy: 71.10%\n",
      "Epoch: 35, Time Spent: 3420.23s, Testing_Accuracy: 70.42%\n",
      "Training loss = 0.2890084033613446 VS Testing loss = 0.29580952380952386\n",
      "----------------------------------------------------------\n",
      "Epoch: 36, Time Spent: 3514.53s, Training_Accuracy: 71.65%\n",
      "Epoch: 36, Time Spent: 3517.19s, Testing_Accuracy: 70.91%\n",
      "Training loss = 0.2835462184873949 VS Testing loss = 0.2908571428571428\n",
      "----------------------------------------------------------\n",
      "Epoch: 37, Time Spent: 3612.42s, Training_Accuracy: 72.20%\n",
      "Epoch: 37, Time Spent: 3615.00s, Testing_Accuracy: 71.37%\n",
      "Training loss = 0.2779663865546218 VS Testing loss = 0.28628571428571425\n",
      "----------------------------------------------------------\n",
      "Epoch: 38, Time Spent: 3711.33s, Training_Accuracy: 72.73%\n",
      "Epoch: 38, Time Spent: 3713.95s, Testing_Accuracy: 71.90%\n",
      "Training loss = 0.27268907563025213 VS Testing loss = 0.2810476190476191\n",
      "----------------------------------------------------------\n",
      "Epoch: 39, Time Spent: 3808.84s, Training_Accuracy: 73.25%\n",
      "Epoch: 39, Time Spent: 3811.40s, Testing_Accuracy: 72.46%\n",
      "Training loss = 0.2674957983193277 VS Testing loss = 0.27542857142857147\n",
      "----------------------------------------------------------\n",
      "Epoch: 40, Time Spent: 3906.42s, Training_Accuracy: 73.80%\n",
      "Epoch: 40, Time Spent: 3909.05s, Testing_Accuracy: 72.97%\n",
      "Training loss = 0.2620336134453781 VS Testing loss = 0.27028571428571424\n",
      "----------------------------------------------------------\n",
      "Epoch: 41, Time Spent: 4004.37s, Training_Accuracy: 74.32%\n",
      "Epoch: 41, Time Spent: 4006.98s, Testing_Accuracy: 73.38%\n",
      "Training loss = 0.2568403361344538 VS Testing loss = 0.2661904761904762\n",
      "----------------------------------------------------------\n",
      "Epoch: 42, Time Spent: 4101.41s, Training_Accuracy: 74.77%\n",
      "Epoch: 42, Time Spent: 4104.04s, Testing_Accuracy: 73.90%\n",
      "Training loss = 0.2522689075630252 VS Testing loss = 0.26095238095238094\n",
      "----------------------------------------------------------\n",
      "Epoch: 43, Time Spent: 4198.86s, Training_Accuracy: 75.20%\n",
      "Epoch: 43, Time Spent: 4201.51s, Testing_Accuracy: 74.27%\n",
      "Training loss = 0.2480336134453781 VS Testing loss = 0.2573333333333333\n",
      "----------------------------------------------------------\n",
      "Epoch: 44, Time Spent: 4296.21s, Training_Accuracy: 75.57%\n",
      "Epoch: 44, Time Spent: 4298.82s, Testing_Accuracy: 74.70%\n",
      "Training loss = 0.24433613445378155 VS Testing loss = 0.25295238095238093\n",
      "----------------------------------------------------------\n",
      "Epoch: 45, Time Spent: 4393.36s, Training_Accuracy: 75.88%\n",
      "Epoch: 45, Time Spent: 4395.89s, Testing_Accuracy: 75.03%\n",
      "Training loss = 0.24121008403361344 VS Testing loss = 0.24971428571428567\n",
      "----------------------------------------------------------\n",
      "Epoch: 46, Time Spent: 4490.87s, Training_Accuracy: 76.15%\n",
      "Epoch: 46, Time Spent: 4493.48s, Testing_Accuracy: 75.32%\n",
      "Training loss = 0.23853781512605043 VS Testing loss = 0.24676190476190474\n",
      "----------------------------------------------------------\n",
      "Epoch: 47, Time Spent: 4588.69s, Training_Accuracy: 76.41%\n",
      "Epoch: 47, Time Spent: 4591.36s, Testing_Accuracy: 75.54%\n",
      "Training loss = 0.23586554621848743 VS Testing loss = 0.24457142857142855\n",
      "----------------------------------------------------------\n",
      "Epoch: 48, Time Spent: 4688.65s, Training_Accuracy: 76.69%\n",
      "Epoch: 48, Time Spent: 4691.26s, Testing_Accuracy: 75.76%\n",
      "Training loss = 0.2331260504201681 VS Testing loss = 0.24238095238095236\n",
      "----------------------------------------------------------\n",
      "Epoch: 49, Time Spent: 4785.80s, Training_Accuracy: 76.89%\n",
      "Epoch: 49, Time Spent: 4788.48s, Testing_Accuracy: 76.09%\n",
      "Training loss = 0.2311428571428571 VS Testing loss = 0.2391428571428571\n",
      "----------------------------------------------------------\n",
      "Epoch: 50, Time Spent: 4882.72s, Training_Accuracy: 77.11%\n",
      "Epoch: 50, Time Spent: 4885.38s, Testing_Accuracy: 76.23%\n",
      "Training loss = 0.2289075630252101 VS Testing loss = 0.23771428571428577\n",
      "----------------------------------------------------------\n",
      "Epoch: 51, Time Spent: 4979.76s, Training_Accuracy: 77.35%\n",
      "Epoch: 51, Time Spent: 4982.40s, Testing_Accuracy: 76.49%\n",
      "Training loss = 0.2265042016806723 VS Testing loss = 0.2351428571428571\n",
      "----------------------------------------------------------\n",
      "Epoch: 52, Time Spent: 5076.70s, Training_Accuracy: 77.57%\n",
      "Epoch: 52, Time Spent: 5079.37s, Testing_Accuracy: 76.77%\n",
      "Training loss = 0.22433613445378153 VS Testing loss = 0.23228571428571432\n",
      "----------------------------------------------------------\n",
      "Epoch: 53, Time Spent: 5173.38s, Training_Accuracy: 77.79%\n",
      "Epoch: 53, Time Spent: 5175.98s, Testing_Accuracy: 77.06%\n",
      "Training loss = 0.22210084033613442 VS Testing loss = 0.22942857142857143\n",
      "----------------------------------------------------------\n",
      "Epoch: 54, Time Spent: 5270.59s, Training_Accuracy: 78.03%\n",
      "Epoch: 54, Time Spent: 5273.24s, Testing_Accuracy: 77.20%\n",
      "Training loss = 0.21974789915966386 VS Testing loss = 0.22799999999999998\n",
      "----------------------------------------------------------\n",
      "Epoch: 55, Time Spent: 5367.90s, Training_Accuracy: 78.15%\n",
      "Epoch: 55, Time Spent: 5370.51s, Testing_Accuracy: 77.40%\n",
      "Training loss = 0.2185042016806723 VS Testing loss = 0.22599999999999998\n",
      "----------------------------------------------------------\n",
      "Epoch: 56, Time Spent: 5465.34s, Training_Accuracy: 78.26%\n",
      "Epoch: 56, Time Spent: 5467.98s, Testing_Accuracy: 77.56%\n",
      "Training loss = 0.2173949579831933 VS Testing loss = 0.22438095238095235\n",
      "----------------------------------------------------------\n",
      "Epoch: 57, Time Spent: 5562.60s, Training_Accuracy: 78.32%\n",
      "Epoch: 57, Time Spent: 5565.22s, Testing_Accuracy: 77.66%\n",
      "Training loss = 0.21684033613445375 VS Testing loss = 0.22342857142857142\n",
      "----------------------------------------------------------\n",
      "Epoch: 58, Time Spent: 5659.31s, Training_Accuracy: 78.32%\n",
      "Epoch: 58, Time Spent: 5661.96s, Testing_Accuracy: 77.76%\n",
      "Training loss = 0.21680672268907564 VS Testing loss = 0.22238095238095235\n",
      "----------------------------------------------------------\n",
      "Epoch: 59, Time Spent: 5756.93s, Training_Accuracy: 78.38%\n",
      "Epoch: 59, Time Spent: 5759.56s, Testing_Accuracy: 77.78%\n",
      "Training loss = 0.21621848739495797 VS Testing loss = 0.22219047619047616\n",
      "----------------------------------------------------------\n",
      "Epoch: 60, Time Spent: 5854.02s, Training_Accuracy: 78.41%\n",
      "Epoch: 60, Time Spent: 5856.64s, Testing_Accuracy: 77.83%\n",
      "Training loss = 0.21593277310924375 VS Testing loss = 0.22171428571428575\n",
      "----------------------------------------------------------\n",
      "Epoch: 61, Time Spent: 5951.24s, Training_Accuracy: 78.41%\n",
      "Epoch: 61, Time Spent: 5953.83s, Testing_Accuracy: 77.86%\n",
      "Training loss = 0.21591596638655464 VS Testing loss = 0.22142857142857142\n",
      "----------------------------------------------------------\n",
      "Epoch: 62, Time Spent: 6049.17s, Training_Accuracy: 78.52%\n",
      "Epoch: 62, Time Spent: 6051.82s, Testing_Accuracy: 77.96%\n",
      "Training loss = 0.21478991596638652 VS Testing loss = 0.22038095238095234\n",
      "----------------------------------------------------------\n",
      "Epoch: 63, Time Spent: 6146.98s, Training_Accuracy: 78.59%\n",
      "Epoch: 63, Time Spent: 6149.70s, Testing_Accuracy: 77.98%\n",
      "Training loss = 0.2140672268907563 VS Testing loss = 0.22019047619047616\n",
      "----------------------------------------------------------\n",
      "Epoch: 64, Time Spent: 6246.90s, Training_Accuracy: 78.59%\n",
      "Epoch: 64, Time Spent: 6249.54s, Testing_Accuracy: 78.01%\n",
      "Training loss = 0.2140672268907563 VS Testing loss = 0.21990476190476194\n",
      "----------------------------------------------------------\n",
      "Epoch: 65, Time Spent: 6344.84s, Training_Accuracy: 78.52%\n",
      "Epoch: 65, Time Spent: 6347.49s, Testing_Accuracy: 78.16%\n",
      "Training loss = 0.21480672268907564 VS Testing loss = 0.21838095238095234\n",
      "----------------------------------------------------------\n",
      "Epoch: 66, Time Spent: 6442.27s, Training_Accuracy: 78.52%\n",
      "Epoch: 66, Time Spent: 6444.85s, Testing_Accuracy: 78.14%\n",
      "Training loss = 0.21482352941176475 VS Testing loss = 0.21857142857142853\n",
      "----------------------------------------------------------\n",
      "Epoch: 67, Time Spent: 6539.50s, Training_Accuracy: 78.47%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67, Time Spent: 6542.13s, Testing_Accuracy: 78.22%\n",
      "Training loss = 0.21527731092436975 VS Testing loss = 0.2178095238095238\n",
      "----------------------------------------------------------\n",
      "Epoch: 68, Time Spent: 6636.79s, Training_Accuracy: 78.35%\n",
      "Epoch: 68, Time Spent: 6639.41s, Testing_Accuracy: 78.15%\n",
      "Training loss = 0.2165042016806723 VS Testing loss = 0.2184761904761905\n",
      "----------------------------------------------------------\n",
      "Epoch: 69, Time Spent: 6734.42s, Training_Accuracy: 78.25%\n",
      "Epoch: 69, Time Spent: 6737.06s, Testing_Accuracy: 78.09%\n",
      "Training loss = 0.21746218487394953 VS Testing loss = 0.2191428571428572\n",
      "----------------------------------------------------------\n",
      "Epoch: 70, Time Spent: 6833.06s, Training_Accuracy: 78.24%\n",
      "Epoch: 70, Time Spent: 6835.67s, Testing_Accuracy: 78.10%\n",
      "Training loss = 0.21757983193277308 VS Testing loss = 0.2189523809523809\n",
      "----------------------------------------------------------\n",
      "Epoch: 71, Time Spent: 6930.83s, Training_Accuracy: 78.20%\n",
      "Epoch: 71, Time Spent: 6933.50s, Testing_Accuracy: 78.06%\n",
      "Training loss = 0.21798319327731097 VS Testing loss = 0.21942857142857142\n",
      "----------------------------------------------------------\n",
      "Epoch: 72, Time Spent: 7028.25s, Training_Accuracy: 78.19%\n",
      "Epoch: 72, Time Spent: 7030.88s, Testing_Accuracy: 78.08%\n",
      "Training loss = 0.21810084033613442 VS Testing loss = 0.21923809523809523\n",
      "----------------------------------------------------------\n",
      "Epoch: 73, Time Spent: 7125.68s, Training_Accuracy: 78.16%\n",
      "Epoch: 73, Time Spent: 7128.35s, Testing_Accuracy: 78.10%\n",
      "Training loss = 0.21838655462184875 VS Testing loss = 0.21904761904761905\n",
      "----------------------------------------------------------\n",
      "Epoch: 74, Time Spent: 7223.50s, Training_Accuracy: 78.16%\n",
      "Epoch: 74, Time Spent: 7226.14s, Testing_Accuracy: 78.19%\n",
      "Training loss = 0.21835294117647064 VS Testing loss = 0.21809523809523812\n",
      "----------------------------------------------------------\n",
      "Epoch: 75, Time Spent: 7320.72s, Training_Accuracy: 78.18%\n",
      "Epoch: 75, Time Spent: 7323.97s, Testing_Accuracy: 78.17%\n",
      "Training loss = 0.21820168067226886 VS Testing loss = 0.2182857142857143\n",
      "----------------------------------------------------------\n",
      "Epoch: 76, Time Spent: 7420.77s, Training_Accuracy: 78.18%\n",
      "Epoch: 76, Time Spent: 7423.45s, Testing_Accuracy: 78.23%\n",
      "Training loss = 0.21816806722689075 VS Testing loss = 0.21771428571428575\n",
      "----------------------------------------------------------\n",
      "Epoch: 77, Time Spent: 7518.26s, Training_Accuracy: 78.23%\n",
      "Epoch: 77, Time Spent: 7520.89s, Testing_Accuracy: 78.18%\n",
      "Training loss = 0.21768067226890753 VS Testing loss = 0.21819047619047616\n",
      "----------------------------------------------------------\n",
      "Epoch: 78, Time Spent: 7615.44s, Training_Accuracy: 78.23%\n",
      "Epoch: 78, Time Spent: 7618.04s, Testing_Accuracy: 78.17%\n",
      "Training loss = 0.21766386554621853 VS Testing loss = 0.2182857142857143\n",
      "----------------------------------------------------------\n",
      "Epoch: 79, Time Spent: 7712.81s, Training_Accuracy: 78.26%\n",
      "Epoch: 79, Time Spent: 7715.47s, Testing_Accuracy: 78.21%\n",
      "Training loss = 0.2174117647058823 VS Testing loss = 0.21790476190476193\n",
      "----------------------------------------------------------\n",
      "Epoch: 80, Time Spent: 7810.60s, Training_Accuracy: 78.32%\n",
      "Epoch: 80, Time Spent: 7813.21s, Testing_Accuracy: 78.28%\n",
      "Training loss = 0.21675630252100841 VS Testing loss = 0.21723809523809523\n",
      "----------------------------------------------------------\n",
      "Epoch: 81, Time Spent: 7908.30s, Training_Accuracy: 78.37%\n",
      "Epoch: 81, Time Spent: 7910.91s, Testing_Accuracy: 78.33%\n",
      "Training loss = 0.21630252100840341 VS Testing loss = 0.21666666666666667\n",
      "----------------------------------------------------------\n",
      "Epoch: 82, Time Spent: 8005.74s, Training_Accuracy: 78.41%\n",
      "Epoch: 82, Time Spent: 8008.40s, Testing_Accuracy: 78.45%\n",
      "Training loss = 0.21591596638655464 VS Testing loss = 0.21552380952380956\n",
      "----------------------------------------------------------\n",
      "Epoch: 83, Time Spent: 8103.20s, Training_Accuracy: 78.46%\n",
      "Epoch: 83, Time Spent: 8105.86s, Testing_Accuracy: 78.63%\n",
      "Training loss = 0.2153949579831933 VS Testing loss = 0.21371428571428575\n",
      "----------------------------------------------------------\n",
      "Epoch: 84, Time Spent: 8201.46s, Training_Accuracy: 78.50%\n",
      "Epoch: 84, Time Spent: 8204.12s, Testing_Accuracy: 78.69%\n",
      "Training loss = 0.2149579831932773 VS Testing loss = 0.2131428571428572\n",
      "----------------------------------------------------------\n",
      "Epoch: 85, Time Spent: 8299.32s, Training_Accuracy: 78.58%\n",
      "Epoch: 85, Time Spent: 8301.96s, Testing_Accuracy: 78.75%\n",
      "Training loss = 0.21415126050420163 VS Testing loss = 0.21247619047619049\n",
      "----------------------------------------------------------\n",
      "Epoch: 86, Time Spent: 8397.23s, Training_Accuracy: 78.66%\n",
      "Epoch: 86, Time Spent: 8399.89s, Testing_Accuracy: 78.82%\n",
      "Training loss = 0.21344537815126052 VS Testing loss = 0.21180952380952378\n",
      "----------------------------------------------------------\n",
      "Epoch: 87, Time Spent: 8494.55s, Training_Accuracy: 78.73%\n",
      "Epoch: 87, Time Spent: 8497.18s, Testing_Accuracy: 78.84%\n",
      "Training loss = 0.2127058823529412 VS Testing loss = 0.2116190476190476\n",
      "----------------------------------------------------------\n",
      "Epoch: 88, Time Spent: 8592.43s, Training_Accuracy: 78.83%\n",
      "Epoch: 88, Time Spent: 8595.08s, Testing_Accuracy: 78.80%\n",
      "Training loss = 0.21168067226890752 VS Testing loss = 0.21199999999999997\n",
      "----------------------------------------------------------\n",
      "Epoch: 89, Time Spent: 8689.89s, Training_Accuracy: 78.87%\n",
      "Epoch: 89, Time Spent: 8692.47s, Testing_Accuracy: 78.87%\n",
      "Training loss = 0.21131092436974785 VS Testing loss = 0.21133333333333337\n",
      "----------------------------------------------------------\n",
      "Epoch: 90, Time Spent: 8786.36s, Training_Accuracy: 78.91%\n",
      "Epoch: 90, Time Spent: 8789.01s, Testing_Accuracy: 78.91%\n",
      "Training loss = 0.21089075630252097 VS Testing loss = 0.21085714285714285\n",
      "----------------------------------------------------------\n",
      "Epoch: 91, Time Spent: 8883.25s, Training_Accuracy: 78.93%\n",
      "Epoch: 91, Time Spent: 8885.91s, Testing_Accuracy: 78.96%\n",
      "Training loss = 0.2107226890756303 VS Testing loss = 0.21038095238095234\n",
      "----------------------------------------------------------\n",
      "Epoch: 92, Time Spent: 8980.63s, Training_Accuracy: 78.94%\n",
      "Epoch: 92, Time Spent: 8983.28s, Testing_Accuracy: 78.99%\n",
      "Training loss = 0.21063865546218485 VS Testing loss = 0.2100952380952381\n",
      "----------------------------------------------------------\n",
      "Epoch: 93, Time Spent: 9077.91s, Training_Accuracy: 79.02%\n",
      "Epoch: 93, Time Spent: 9080.46s, Testing_Accuracy: 79.04%\n",
      "Training loss = 0.20976470588235296 VS Testing loss = 0.2096190476190476\n",
      "----------------------------------------------------------\n",
      "Epoch: 94, Time Spent: 9175.58s, Training_Accuracy: 79.11%\n",
      "Epoch: 94, Time Spent: 9178.22s, Testing_Accuracy: 79.08%\n",
      "Training loss = 0.20890756302521007 VS Testing loss = 0.20923809523809522\n",
      "----------------------------------------------------------\n",
      "Epoch: 95, Time Spent: 9272.54s, Training_Accuracy: 79.17%\n",
      "Epoch: 95, Time Spent: 9275.12s, Testing_Accuracy: 79.13%\n",
      "Training loss = 0.2083025210084034 VS Testing loss = 0.20866666666666667\n",
      "----------------------------------------------------------\n",
      "Epoch: 96, Time Spent: 9372.28s, Training_Accuracy: 79.27%\n",
      "Epoch: 96, Time Spent: 9374.84s, Testing_Accuracy: 79.19%\n",
      "Training loss = 0.20732773109243696 VS Testing loss = 0.2080952380952381\n",
      "----------------------------------------------------------\n",
      "Epoch: 97, Time Spent: 9470.39s, Training_Accuracy: 79.39%\n",
      "Epoch: 97, Time Spent: 9473.04s, Testing_Accuracy: 79.28%\n",
      "Training loss = 0.20605042016806718 VS Testing loss = 0.20723809523809522\n",
      "----------------------------------------------------------\n",
      "Epoch: 98, Time Spent: 9568.41s, Training_Accuracy: 79.47%\n",
      "Epoch: 98, Time Spent: 9571.03s, Testing_Accuracy: 79.30%\n",
      "Training loss = 0.20527731092436974 VS Testing loss = 0.206952380952381\n",
      "----------------------------------------------------------\n",
      "Epoch: 99, Time Spent: 9665.83s, Training_Accuracy: 79.56%\n",
      "Epoch: 99, Time Spent: 9668.47s, Testing_Accuracy: 79.34%\n",
      "Training loss = 0.20436974789915963 VS Testing loss = 0.20657142857142852\n",
      "----------------------------------------------------------\n",
      "Epoch: 100, Time Spent: 9764.14s, Training_Accuracy: 79.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Time Spent: 9766.74s, Testing_Accuracy: 79.38%\n",
      "Training loss = 0.20349579831932774 VS Testing loss = 0.20619047619047615\n",
      "----------------------------------------------------------\n",
      "[0.7311428571428571, 0.6245042016806723, 0.5370420168067227, 0.49635294117647055, 0.49329411764705877, 0.48347899159663865, 0.46821848739495797, 0.4483697478991596, 0.4263361344537815, 0.40956302521008403, 0.39672268907563024, 0.3876974789915967, 0.380672268907563, 0.3744873949579832, 0.3693613445378151, 0.3660168067226891, 0.3631932773109243, 0.359764705882353, 0.35680672268907565, 0.35307563025210087, 0.34929411764705887, 0.3458151260504202, 0.34221848739495797, 0.3386722689075631, 0.3351260504201681, 0.3307058823529412, 0.3269747899159664, 0.32262184873949584, 0.3186050420168067, 0.3139159663865546, 0.3098319327731093, 0.3043193277310924, 0.2990252100840336, 0.29443697478991593, 0.2890084033613446, 0.2835462184873949, 0.2779663865546218, 0.27268907563025213, 0.2674957983193277, 0.2620336134453781, 0.2568403361344538, 0.2522689075630252, 0.2480336134453781, 0.24433613445378155, 0.24121008403361344, 0.23853781512605043, 0.23586554621848743, 0.2331260504201681, 0.2311428571428571, 0.2289075630252101, 0.2265042016806723, 0.22433613445378153, 0.22210084033613442, 0.21974789915966386, 0.2185042016806723, 0.2173949579831933, 0.21684033613445375, 0.21680672268907564, 0.21621848739495797, 0.21593277310924375, 0.21591596638655464, 0.21478991596638652, 0.2140672268907563, 0.2140672268907563, 0.21480672268907564, 0.21482352941176475, 0.21527731092436975, 0.2165042016806723, 0.21746218487394953, 0.21757983193277308, 0.21798319327731097, 0.21810084033613442, 0.21838655462184875, 0.21835294117647064, 0.21820168067226886, 0.21816806722689075, 0.21768067226890753, 0.21766386554621853, 0.2174117647058823, 0.21675630252100841, 0.21630252100840341, 0.21591596638655464, 0.2153949579831933, 0.2149579831932773, 0.21415126050420163, 0.21344537815126052, 0.2127058823529412, 0.21168067226890752, 0.21131092436974785, 0.21089075630252097, 0.2107226890756303, 0.21063865546218485, 0.20976470588235296, 0.20890756302521007, 0.2083025210084034, 0.20732773109243696, 0.20605042016806718, 0.20527731092436974, 0.20436974789915963, 0.20349579831932774]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score is 0.7628661260964605\n"
     ]
    }
   ],
   "source": [
    "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])\n",
    "predictions,loss = dnn.train(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
